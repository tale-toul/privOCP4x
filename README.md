# PRIVATE OCP 4 cluster on AWS

## Table of contents

* [Introduction](#introduction)
* [Full cluster installation](#full-cluster-installation)
* [Deploying a cluster in an existing VPC](#Deploying-a-cluster-in-an-existing-VPC)
* [VPC creation](#vpc-creation)
  * [Terraform installation](#terraform-installation)
    * [Variables](#variables)
    * [Endpoints](#endpoints)
    * [Proxy configuration](#proxy-configuration)
  * [Deploying the infrastructure with terraform](#deploying-the-infrastructure-with-terraform)
* [Bastion setup with Ansible](#bastion-setup-with-ansible)
  * [Proxy configuration](#proxy-configuration)
    * [Running the ansible playbook](#running-the-ansible-playbook)
    * [Template constructions](#template-constructions)
* [OCP cluster deployment](#ocp-cluster-deployment)
* [Cluster decommissioning instructions](#cluster-decommissioning-instructions)
* [Accessing the cluster](#accessing-the-cluster)
  * [Turning the private cluster public](#turning-the-private-cluster-public)

## Introduction

Deploy a private OCP 4 cluster in an existing VPC on AWS.  The terraform template creates the VPC infrastructure, although a pre-existing VPC can be used.

The cluster being private is not directly accessible from the Internet, application and API load balancers are created in the private subnets, the connections from the cluster to the Internet can be configured via NAT gateways or via a proxy server running in a bastion host. 

This project has been tested up to version OCP 4.11

[Reference documentation](https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-private.html#installing-aws-private)

## Full cluster installation

The installation of the whole cluster is divided in 3 steps, click the following links in order to go to the specific sections: 

* [Create VPC insfrastructure with terraform](#deploying-the-infrastructure-with-terraform)

* [Set up the bastion host](#running-the-ansible-playbook)

* [Install Openshift](#ocp-cluster-deployment)

## Deploying a cluster in an existing VPC

If the cluster is to be deployed in an existing VPC, possibly sharing it with other clusters, the terraform creation part will be skipped and only the Ansible part will be run.

One thing to keep in mind is that two or more cluster can be installed in the same VPC, but the DNS zones must be different for each one if these are OCP v4 public clusters. The clusters can share the same VPC and DNS zone if one of them is v3 and the other is v4.  A private OCP v4 cluster could be deployed on a different VPC using the same private DNS zone as another OCP v4 cluster, because the private cluster does not create public resources.  


The basic requirements are:

* The following ansible variables must be defined either in a file located in the directory **Ansible/group_vars/all/**, the name of the file is not important; or appended as extra variables to the ansible command.  When creating the infrastructure with terraform, most of these variables are defined as output vars, but in this case they need to be provided by other means to the ansible playbook that prepares the cluster installation environment:

  * terraform_created.- Boolean defining if the infrastructure was created by terraform and therefore ansible can read the variables from it.  Default values is true.  Use _false_ when deploying on an existing VPC:
```
terraform_created: true
```
  * base_dns_domain.- String defining the base DNS domain of your cloud provider.  The full DNS name for your cluster is a combination of the **base domain** and *cluster_name* parameters that uses the <cluster_name>.<baseDomain> format.  The subdomain will be created but the parent domain must exist, for example for abbyext.example.com, example.com must already exist, and abbyext will be created. The DNS zone <cluster_name>.<baseDomain> is a private zone, records will be added to this zone only, not the public **base domain**.  This variable by default is generated by Terraform:
```
base_dns_domain : "abbyext.example.com"
```
  * enable_proxy.- Boolean defining if a proxy will be setup as the only means to access the Internet from the cluster.  Default is false, no proxy is created and the cluster will try to access the Internet directly.  If set to true a proxy will be setup on the bastion and the cluster will access the Internet through it.  This variable by default is generated by Terraform:
```
enable_proxy : false
```
  * availability_zones.- List of availability zones where the VPC has public and private subnets and where cluster nodes will be located.  This list does not need to include all availability zones, only the ones where nodes will be placed.  This variable by default is generated by Terraform:
```
availability_zones : [
  "eu-west-2a",
  "eu-west-2b",
  "eu-west-2c",
]
```
  * cluster_name.- String defining a name for the cluster.  This variable by default is generated by Terraform:
```
cluster_name : "abbyext"
```
  * vpc_cidr.- Network address space of the VPC. This variable by default is generated by Terraform:
```
vpc_cidr : "172.20.0.0/16"
```
  * region_name.- The AWS region name where the VPC resides. This variable by default is generated by Terraform:
```
region_name : "eu-west-2"
```
  * private_subnets.- List of _private_ subnet ids already existing in the VPC where cluster components will be created. These subnets must exist in the availability zones declared with the variable *availability_zones* in the same command line.  This variable by default is generated by Terraform:
```
private_subnets : [
  "subnet-02fd813cfffe1b838",
  "subnet-028c6f7b62139adb9",
  "subnet-03974dac9438ff8da",
]
```
  * ssh_keyfile.- Name of the file containing the public part of the ssh key to deploy to the bastion host:
```
ssh_keyfile: ocp-ssh.pub
```

An example execution with the variables defined on the command like follows:

```shell
$ ansible-playbook -i inventory privsetup.yaml --vault-id vault-id -e terraform_created=false -e base_dns_domain=example.com -e enable_proxy=false -e '{"availability_zones": ["eu-west-1a","eu-west-1b"]}' -e cluster_name=rhpnt -e vpc_cidr="172.20.0.0/16" -e region_name="eu-west-1" -e '{"private_subnets":["subnet-0ea3ec602f2e0baee", "subnet-0b032d4c5b631a6ea"]}'
```
When the variables are defined follow the instruction in the following sections:

* [Set up the bastion host](#running-the-ansible-playbook)

* [Install Openshift](#ocp-cluster-deployment)


## VPC creation

Create a VPC in AWS using **terraform** to deploy a private OCP 4 cluster on it.

In addition to the VPC network components, a bastion host in a public subnet inside the private VPC is required to run the intallation program from it; the bastion is placed in a public subnet so it can be accessed via ssh, and the Openshift installer is run from this bastion host so the names in the private DNS domain can be resolved.  This bastion host can also take the role of proxy server for the cluster nodes in the private subnets.

### Terraform installation

[Terraform](https://www.terraform.io/) must be installed in the local host.
  The terraform installation is straight forward, just follow the instructions for your operating system in the [terraform site](https://www.terraform.io/downloads.html)

Verify that terraform is working:

```shell
 # terraform --version
```

### Variables

All input variables and locals are defined in a separate file _Teraform/input-vars.tf_.  This file can be used as reference to know what components of the VPC or bastion can be specified at the time of creation.

### Endpoints

A best practice when deploying the VPC is creating endpoints for all the AWS services that are used by the OCP cluster, this will improve security and speed since the communications between the cluster and these services never leave the AWS internal network.  The use of endpoints is a must in case the cluster only access to the Internet is via a proxy server.

The AWS services used by OCP and available as endpoints are: 

* **s3**.- Of type Gateway, is associated with all route tables defined in subnets where it will be used.
* **ec2**.- Of type Interface, requires private dns enabled, is associated with the subnets where it will be used, with the limitation of only one subnet per availability zone.  Also security groups must be assigned to them to define what ports are allowed from where.
* **elastic load balancing**.- Of type Interface, requires private dns enabled, is associated with the subnets where it will be used, with the limitation of only one subnet per availability zone.  Also security groups must be assigned to them to define what ports are allowed from where.


### Proxy configuration

If the access from the cluster nodes to the Internet will be routed through a proxy server the variable **enable_proxy** must be set to true, by default is false.  This variable is used in several conditional expressions to decide on the configuration of some components:

* The security groups assigned to the bastion host.- If the proxy is enabled, a security group for ingress port 3128 is created and later added to the bastion, this port is where squid proxy provides its service. 

```
resource "aws_security_group" "sg-squid" {
    count = var.enable_proxy ? 1 : 0
...
bastion_security_groups = var.enable_proxy ? concat([aws_security_group.sg-ssh-in.id, aws_security_group.sg-all-out.id], aws_security_group.sg-squid[*].id) : [aws_security_group.sg-ssh-in.id, aws_security_group.sg-all-out.id]
...
resource "aws_instance" "tale_bastion" {
  ami = var.rhel-ami[var.region_name]
  instance_type = "m4.large"
  subnet_id = aws_subnet.subnet_pub.0.id
  vpc_security_group_ids = local.bastion_security_groups
```

* Public subnets.- If the proxy is enabled only one public subnet is created to place the bastion host, if not enabled as many public as private subnets are created:

```
public_subnet_count = var.enable_proxy ? 1 : local.private_subnet_count
```
* NAT gateways.- If the proxy is enable NAT gateways, its elastic IPs and the route to use them will not be created, since all the Internet bound connections will go through the proxy.
```
resource "aws_eip" "nateip" {
  count = var.enable_proxy ? 0 : local.public_subnet_count
...
resource "aws_nat_gateway" "natgw" {
    count = var.enable_proxy ? 0 : local.public_subnet_count
...
resource "aws_route" "internet_access" {
  count = var.enable_proxy ? 0 : local.private_subnet_count
  route_table_id = aws_route_table.rtable_priv[count.index].id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id = aws_nat_gateway.natgw[count.index].id
}
```

### Deploying the infrastructure with terraform

Terraform is used to create the infrastructure components of the VPC, some of these components can be adjusted via the use of variables defined in the file _Terrafomr/input-vars.tf_ like: 

* **region_name**.- The AWS region where the infrastructure is created

     Default value: eu-west-1

* **dns_domain_ID**.- This is the Hosted zone ID in route53 for the public DNS zone where the bastion hostname is created

* **domain_name**.- Public DNS subdomain to access bastion host DNS name.  The full public DNS domain is built as domain_name + dns_domain_ID.name.  For domain_name=tale and dns_domain_ID.name=redhat.com, the full DNS domain is tale.redhat.com.  This subdomain is created by terraform if it does not exist.

     Default value: tale

* **cluster_name**.- Used in the install-config file to define a name for the cluster.

     Default value: ocp

* **ssh_keyfile**.- Name of the file with public part of the SSH key to transfer to the EC2 instances

     Default value: ocp-ssh.pub

* **enable_proxy**.- Boolean value to determine if a proxy is to be deployed (true) or not (false).  If the proxy is enabled, all outgoing communications from the cluster will go through it.  The proxy is later set up with ansible.

     Default value: false

Check for any updates in the terraform plugins:

```shell
  $ cd Terraform
  $ terraform init
  
  Initializing the backend...
...  
```
The private full DNS domain for the cluster is created by adding the cluste name to the external domain name.  The private internal and external domains are called the same.  For example if cluster_name=ocp and the domain associated with dns_domain_ID is redhat.com, the full DNS private domain is ocp.redhat.com.

Make sure the aws account credentials are defined in the file $HOME/.aws/credentials or in the environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY as explained in terraform [documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-configuration-and-credentials-files)

Run the terraform apply command with the variable values desired:

```shell
terraform apply -var="subnet_count=2" -var="domain_name=kali" -var="cluster_name=olivkaj" -var="enable_proxy=true"
```
Save the value of the variables used in this step because the same values are required in case the infrastructure is later destroyed with the **terrafor destroy** command.  In the example the use of !! assumes that no other command has been executed after _terraform apply_:

```
$ echo "!!" > terraform_apply.txt
```

Alternatively the variable assigments can be defined in a file 
```
cat ocp_priv.vars 
region_name = "eu-west-3"
domain_name = "eagle"
cluster_name = "amiga"
ssh_keyfile = "upi-ssh.pub"
```
The file can be called in the terraform command with the -var-file option:
```
terraform apply -var-file ocp_priv.vars
```

## Bastion setup with Ansible

To successfully deploy the cluster some elements are required besides the AWS infrastructure created earlier:

* Permanent credentials for an AWS account

* An ssh key pair (Optional).- the public part of the key will be installed on every node in the cluster so it is possible to connect to them via ssh.

* A DNS base domain.- This can be a public or private domain.  A private subdomain will be created under the base domain and all DNS recrods created during installation will be created in the private subdomain 

* Pull secret.- The pull secret can be obtained [here](https://cloud.redhat.com/openshift/install)

* Openshift Installer.- This can be downloaded from the same [site](https://cloud.redhat.com/openshift/install)

The setup process can be automated using the ansible playbook **privsetup.yaml**, this playbook sets up the bastion host created by terraform.

### Proxy configuration

The same variable used by terraform to enable the proxy is used by ansible, read from the output variables stored in the file *Ansible/group_vars/all/terraform_outputs.var*.  If this boolean variable is set to true, a block of tasks is executed to install, setup and enable the proxy squid service.  The setup of squid just consists of adding an ACL line with the network range of the VPC, so any host with an IP in the VPC can access the Internet through the proxy, no authentication is required:
```
 - name: Add localnet to squid config file
   lineinfile:
     path: /etc/squid/squid.conf
     insertafter: '^acl localnet'
     line: 'acl localnet src {{ vpc_cidr }} #Included by Ansible for VPC access'
```

The install-config.j2 template also contains a conditional block to add the proxy configuration if the *enable_proxy* variable is enabled
```
{% if enable_proxy|bool %}
proxy:
  httpProxy: http://{{bastion_private_ip}}:3128
  httpsProxy: http://{{bastion_private_ip}}:3128
  noProxy: {{ vpc_cidr }}
{% endif %}
```

#### Running the ansible playbook

Review the file **group_vars/all/cluster-vars** and modify the value of the variables to the requirements for the cluster:

* **terraform_created**.- Boolean signaling that the infrastructure was created by terraform and the output variables it generates can be used by ansible. By default is true, when false, ansible will not try to read terraform output vars.
* **compute_nodes**.- number of compute nodes to create, by default 3
* **compute_instance_type**.- The type of AWS instance that will be used to create the compute nodes, by default m4.large 
* **master_nodes**.- number of master nodes to create, by default 3
* **master_instance_type**.- The type of AWS instance that will be used to create the master nodes, by default m4.large m4.xlarge 
* **ocp_version**.- Ther Openshift version to be installed

Download the pull secret from [here](https://cloud.redhat.com/openshift/install) and save in a file called pull-secret in the Ansible directory.

Add the private ssh key associated with the public key used by terraform to the ssh agent:

```shell
ssh-add ../Terraform/ocp-ssh
```

The inventory file is updated by the ansible playbook, there is no need to create one.
```
touch inventory
```

Run the playbook:

```shell
$ ansible-playbook -vvv -i inventory privsetup.yaml --vault-id vault-id
```

#### Template constructions 

The template used to create the install-config.yaml configuration file uses some advance contructions:

* Regular expresion filter.- The base_dns_domain variable from terraform includes a dot (.) at the end, that has to be removed, otherwise the cluster installation fails, for that a regular expresion filter is used:

```
baseDomain: {{ base_dns_domain | regex_replace('(.*)\.$' '\\1') }}
```

* for loops.- The variable containing the values is *availability_zones*, it comes from terraform and ansible understands it as a list in its original form, except for the substitution of the equal sign for the colom:

```
availability_zones : [
  "eu-west-1a",
  "eu-west-1b",
]
```

```
{% for item in availability_zones %}
        - {{ item }}
{% endfor %}
```
* Content from another file.- The pull secret and ssh key is loaded from another file:

```
pullSecret: '{{ lookup('file', './pull-secret') }}'
```


## OCP cluster deployment

Once the ansible playbook has completed successfully, ssh into the bastion host to run the cluster installation.  The installation must be executed from a host in the same VPC that was created by terraform, otherwise it will not be able to resolve the internal DNS names of the components or even access the API endpoint.

To get the FQDNS name of the bastion run the command:
```
terraform output bastion_dns_name
 "bastion.gengi.example.com"
```
Now ssh into the bastion host, using the private ssh key for the public key injected into the nodes:
```
ssh -i ocp-ssh ec2-user@bastion.gengi.example.com
```

Make a backup copy of the install-config.yaml file, because the installer destroyes it when it runs.

```shell
cp privOCP4/ocp4/install-config.yaml .
```
Run the installer as in the example bellow, it will prompt for the AWS credentials that it requires to create AWS resources, after that the installer proceed with the OCP installation:

```shell
$ openshift-install create cluster --dir privOCP4/ocp4 --log-level=info
? AWS Access Key ID [? for help] XXXXX
? AWS Secret Access Key [? for help] ****************************************
INFO Writing AWS credentials to "/home/ec2-user/.aws/credentials" (https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) 
INFO Consuming Install Config from target directory 
INFO Creating infrastructure resources...
...
```

## Cluster decommissioning instructions

Deleting the cluster is a two step process:

* Delete the components created by the openshift-install binary, run this command from the same bastion host and directory from where the installation was run:

```shell
$ openshift-install destroy cluster --dir privOCP4/ocp4 --log-level=info
```

* Delete the components created by terraform,  use the `terraform destroy` command.  This command should include the same variable definitions that were used during cluster creation, or reference the variable file if one was used.   This command is run from the same host and directory from which the `terraform apply` command was run:

```shell
cd Terraform
terraform destroy -var="subnet_count=2" -var="domain_name=kali" -var="cluster_name=olivkaj" -var="enable_proxy=true"
#or
terraform destroy -var-file ocp_priv.vars
```

## Accessing the cluster

Once the cluster is up and running, it is only accessible from inside the VPC, for example from the bastion host, using the *oc* client copied into the privOCP4 directory.

It is possible to access the cluster web UI and applications from outside the VPC by creating a temporary ssh tunnel through the bastion host to the internal applications load balancer.  
Create a tunnel from a host outside the VPC, through the bastion, to the internal apps load balancer with the following commands.  Since the starting point of the tunnel uses priviledged ports, the commands must be run as root, running the commands with sudo does not work.  The ssh private key added to the session must be the same one injected into the nodes by terraform.  The IP 172.20.148.245 in the example is that of the applications load balancer. Any hostname in the apps subdomain is valid:

```
 $ su -
 # ssh-agent bash
 # ssh-add Terraform/ocp-ssh
 # ssh -fN -L localhost:80:172.20.148.245:80 ec2-user@bastion.gengi.example.com
 # ssh -fN -L localhost:443:172.20.148.245::443 ec2-user@bastion.gengi.example.com
```
Next, add entries to /etc/hosts with the names that will be used to access the URL, for example to access the web console: 
```
127.0.0.1 console-openshift-console.apps.lentisco.tangai.rhcee.support
127.0.0.1 oauth-openshift.apps.lentisco.tangai.rhcee.support
```
Now it is possible to access the cluster's web console from the local host using the URL `https://console-openshift-console.apps.lentisco.tangai.rhcee.support`

### Turning the private cluster public

It is possible to modify the configuration of the OCP cluster to be able to access the applications from the Internet in a permanent way, without the need of tricks like ssh tunnels.

The procedure consists on replacing the internal applications load balancer created by the control plane during installation by a public applications load balancer, and also adding a DNS entry **.apps.[cluster name]** to a public DNS zone hosting the cluster base domain.

* **Create public subnets**.- If the cluster does not already have a public subnet in every availability zone where a private subnet already exist, these must be created, each public subnet must get a CIDR network space from the VPC CIDR space, that is available and not used by another subnet.  Each public subnet must have an association with a route table that has a default route to the VPC's Internet Gateway.

* **Put the public subnets under OCP control**.- To make the cluster aware of the public subnets, a particular tag must be added to the subnets.  The tag is created for most of the AWS resources during cluster installation and has the format **kubernetes.io/cluster/[clustername]-[random string]=shared**.  The particular value for a cluster can be obtainend from the existing private subnets.  This same tag must be added to the public subnets.

* **Create default ingress controller manifest**.- The default ingress controller provides access to the applications deployed in the cluster and accessed under the DNS domain _*.apps_.  One of the components managed by the ingress controller is an applications load balancer, in the case of a private cluster this load balancer is created in the private subnets and is not accesible from the Internet.  

  Extract the default ingress controller configuration:

```shell
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml > default-ingress-controller.yaml
```

  Edit the file and remove the whole status section, and in the metadate section just leave the name and namespace entries.  The result should look similar to this:

```yaml
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: Internal
    type: LoadBalancerService
```
 
  Modify the __scope__ entry in the yaml definition and replace __Internal__ by __External__.  The scope section tells the ingress operator where to create the load balancer, whether in the private or public subnets inside the VPC.

* **Replace the ingress controller**.- Run the following command as an administrator, the execution takes a couple minutes while the control plane deletes the internal load balancer and creates a new external one.

```shell
 $ oc replace --force --wait -f default-ingress-controller.yaml
 ingresscontroller.operator.openshift.io "default" deleted
 ingresscontroller.operator.openshift.io/default replaced
```

  The events in the openshift-ingress and openshift-ingress-operator namespaces, and the logs in the ingress-operator deployment should show the actions being taken to replace the ingress controller.   

```shell
 $ oc get events -w -n openshift-ingress-operator
 $ oc get events -w -n openshift-ingress
 $ oc logs -f deployment/ingress-operator -c ingress-operator -n openshift-ingress-operator
```
 Check the status section of the new ingress controller and verify that all conditions are as expected:
```shell
 $ oc describe ingresscontroller default -n openshift-ingress-operator
```
  A new applications load balancer must exist now, and the old one has been deleted, check it with AWS cli or web console.

* **DNS configuration**.- Add a public DNS entry with the format __*.apps.[cluster name]__ and value aliased to the DNS name of the just created public load balancer to the _base domain_ DNS public zone.  If the _base domain_ zone is not public, a new public zone with the same name must be created, otherwise the applications DNS names will not be resolvable from the Internet.  Note that the __*.apps.[cluster name]__ entry is created in the _base domain_ public zone, not in the __[cluster name].[base domain]__ private zone.  Now the cluster applications can be accessed from the Internet, including the cluster web console.

